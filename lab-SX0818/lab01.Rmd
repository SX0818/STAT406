---
title: "Lab 01"
author: "[your name here]"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
    number_sections: false
    toc: FALSE
---

# Instructions



# Brief description


In the lectures , you learned the basics in evaluating the
predictive ability of a model by estimating the mean squared prediction
error. Two common approaches are (a) using a test set, and (b) cross
validation. In this lab, we will work on the prostate cancer data set.
The (modified) data set has 96 observations and 9 variables; the
variable `lpsa` in the 9th column is the response while the rest are
potential predictors. The data set is available in the `{ElemStatLearn}` package and a
description can be found at
<http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.info.txt>


# Questions

Load the data into `R` and remove the `train` column.

```{r}
data(prostate, package = "ElemStatLearn")
prostate <- subset(prostate, select = -train)
```

You can view the first few observations by typing `head(dat)`. In this
lab, we are interested in two linear models: 
a. The **[full model]** with all 8 predictors, and 
a. The **[reduced model]** with the 3 predictors most correlated with the response `lpsa`.

## Q1

Which 3 predictors are the most correlated with `lpsa`?

::: {.solbox data-latex=""}
```{r}
# some code

```
:::

## Q2

Without fitting the linear models, can you tell which model fits the data better in the sense of having a smaller residual sum of squares? Why?

::: {.solbox data-latex=""}
Some text.

:::

Next, we estimate the prediction error using the test set approach as
well as cross validation. Enter the following command in R to generate a random sequence of group indices (group 1, 2 or 3) for the observations.

**Note: Using another random seed, or not using one, or running the
`sample` command more than once will give you a different sequence than
the one we will use for marking this worksheet --- make sure you get
this right**.

```{r}
set.seed(400)
gpind <- sample(rep(1:3, each = 32))
```

The first 6 elements of `gpind` should read `r gpind[1:6]`.

## Q3

Fit the full and reduced models using observations
**[not]** in group 1, and then obtain predictions for those in
group 1. What are the mean squared prediction errors for the two
models?

**Hint**: The reduced model uses the 3 predictors most correlated
with the response in the training set.

::: {.solbox data-latex=""}
```{r}
# some code


```
:::


## Q4

Does your result in **Q3** contradict with that in **Q2**, and why?

::: {.solbox data-latex=""}    
Some text.

:::

## Q5

Repeat the procedure in **Q3** for groups 2 and 3, so as to perform a 3-fold cross validation. What are the overall mean squared prediction errors for the two models?

::: {.solbox data-latex=""}
```{r}
# some code


```
:::

## Q6

Based on these results, which model has a better
predictive ability? Suggest one advantage of the $K$-fold cross
validation ($K>1$) over the leave-one-out cross validation.

::: {.solbox data-latex=""}    
Some text.

:::
